
==> Audit <==
|------------|-----------------|----------|--------|---------|----------------------|----------------------|
|  Command   |      Args       | Profile  |  User  | Version |      Start Time      |       End Time       |
|------------|-----------------|----------|--------|---------|----------------------|----------------------|
| start      | --driver=docker | minikube | joanna | v1.35.0 | 14 Apr 25 14:16 CEST | 14 Apr 25 14:18 CEST |
| service    | hello-node      | minikube | joanna | v1.35.0 | 14 Apr 25 14:25 CEST |                      |
| service    | hello-node      | minikube | joanna | v1.35.0 | 14 Apr 25 14:29 CEST | 14 Apr 25 14:32 CEST |
| service    | hello-node      | minikube | joanna | v1.35.0 | 14 Apr 25 14:33 CEST | 14 Apr 25 14:36 CEST |
| service    | hello-node      | minikube | joanna | v1.35.0 | 14 Apr 25 14:38 CEST | 14 Apr 25 14:42 CEST |
| service    | hello-node      | minikube | joanna | v1.35.0 | 14 Apr 25 14:43 CEST | 14 Apr 25 14:44 CEST |
| service    | hello-node      | minikube | joanna | v1.35.0 | 14 Apr 25 14:45 CEST | 14 Apr 25 14:47 CEST |
| service    | nginx-hello     | minikube | joanna | v1.35.0 | 14 Apr 25 14:47 CEST | 14 Apr 25 14:49 CEST |
| service    | nginx-hello     | minikube | joanna | v1.35.0 | 14 Apr 25 14:49 CEST | 14 Apr 25 15:01 CEST |
| dashboard  |                 | minikube | joanna | v1.35.0 | 14 Apr 25 14:55 CEST |                      |
| docker-env |                 | minikube | joanna | v1.35.0 | 14 Apr 25 14:58 CEST | 14 Apr 25 14:58 CEST |
| service    | joanna-web      | minikube | joanna | v1.35.0 | 14 Apr 25 14:59 CEST |                      |
| docker-env |                 | minikube | joanna | v1.35.0 | 14 Apr 25 15:00 CEST | 14 Apr 25 15:00 CEST |
| service    | joanna-web      | minikube | joanna | v1.35.0 | 14 Apr 25 15:02 CEST |                      |
| docker-env |                 | minikube | joanna | v1.35.0 | 14 Apr 25 15:03 CEST | 14 Apr 25 15:03 CEST |
| service    | joanna-web      | minikube | joanna | v1.35.0 | 14 Apr 25 15:04 CEST |                      |
| docker-env |                 | minikube | joanna | v1.35.0 | 14 Apr 25 15:05 CEST | 14 Apr 25 15:05 CEST |
| service    | joanna-web      | minikube | joanna | v1.35.0 | 14 Apr 25 15:07 CEST |                      |
|------------|-----------------|----------|--------|---------|----------------------|----------------------|


==> Last Start <==
Log file created at: 2025/04/14 14:16:02
Running on machine: Joanna
Binary: Built with gc go1.23.4 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0414 14:16:02.627443    1075 out.go:345] Setting OutFile to fd 1 ...
I0414 14:16:02.627603    1075 out.go:397] isatty.IsTerminal(1) = true
I0414 14:16:02.627607    1075 out.go:358] Setting ErrFile to fd 2...
I0414 14:16:02.627610    1075 out.go:397] isatty.IsTerminal(2) = true
I0414 14:16:02.627796    1075 root.go:338] Updating PATH: /home/joanna/.minikube/bin
W0414 14:16:02.627888    1075 root.go:314] Error reading config file at /home/joanna/.minikube/config/config.json: open /home/joanna/.minikube/config/config.json: no such file or directory
I0414 14:16:02.628753    1075 out.go:352] Setting JSON to false
I0414 14:16:02.630163    1075 start.go:129] hostinfo: {"hostname":"Joanna","uptime":722,"bootTime":1744632241,"procs":32,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"5.15.167.4-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"b4eae882-1c49-4e89-a7a8-776db1ef01c2"}
I0414 14:16:02.630207    1075 start.go:139] virtualization:  guest
I0414 14:16:02.632002    1075 out.go:177] 😄  minikube v1.35.0 on Ubuntu 24.04 (amd64)
W0414 14:16:02.634485    1075 preload.go:293] Failed to list preload files: open /home/joanna/.minikube/cache/preloaded-tarball: no such file or directory
I0414 14:16:02.634540    1075 notify.go:220] Checking for updates...
I0414 14:16:02.634589    1075 driver.go:394] Setting default libvirt URI to qemu:///system
I0414 14:16:02.744654    1075 docker.go:123] docker version: linux-28.0.4:Docker Desktop 4.0.0 ()
I0414 14:16:02.744750    1075 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0414 14:16:03.954990    1075 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.210217373s)
I0414 14:16:03.955360    1075 info.go:266] docker info: {ID:0b840456-f410-42bf-b46b-d3e966ceefc3 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:0 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:57 OomKillDisable:true NGoroutines:59 SystemTime:2025-04-14 12:16:03.946460519 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:7 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:16630759424 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/usr/local/lib/docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:/usr/local/lib/docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
I0414 14:16:03.955487    1075 docker.go:318] overlay module found
I0414 14:16:03.957831    1075 out.go:177] ✨  Using the docker driver based on user configuration
I0414 14:16:03.960141    1075 start.go:297] selected driver: docker
I0414 14:16:03.960147    1075 start.go:901] validating driver "docker" against <nil>
I0414 14:16:03.960156    1075 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0414 14:16:03.960228    1075 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0414 14:16:04.139258    1075 info.go:266] docker info: {ID:0b840456-f410-42bf-b46b-d3e966ceefc3 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:0 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:57 OomKillDisable:true NGoroutines:59 SystemTime:2025-04-14 12:16:04.13080116 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:7 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:16630759424 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/usr/local/lib/docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:/usr/local/lib/docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
I0414 14:16:04.139384    1075 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0414 14:16:04.139964    1075 start_flags.go:393] Using suggested 3900MB memory alloc based on sys=15860MB, container=15860MB
I0414 14:16:04.140748    1075 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0414 14:16:04.142327    1075 out.go:177] 📌  Using Docker driver with root privileges
W0414 14:16:04.143756    1075 out.go:270] ❗  For an improved experience it's recommended to use Docker Engine instead of Docker Desktop.
Docker Engine installation instructions: https://docs.docker.com/engine/install/#server
I0414 14:16:04.143805    1075 cni.go:84] Creating CNI manager for ""
I0414 14:16:04.143847    1075 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0414 14:16:04.143852    1075 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0414 14:16:04.143905    1075 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/joanna:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0414 14:16:04.146312    1075 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0414 14:16:04.147529    1075 cache.go:121] Beginning downloading kic base image for docker with docker
I0414 14:16:04.148838    1075 out.go:177] 🚜  Pulling base image v0.0.46 ...
I0414 14:16:04.150092    1075 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0414 14:16:04.150199    1075 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0414 14:16:04.166374    1075 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0414 14:16:04.167028    1075 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0414 14:16:04.167266    1075 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0414 14:16:04.305098    1075 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0414 14:16:04.305118    1075 cache.go:56] Caching tarball of preloaded images
I0414 14:16:04.305405    1075 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0414 14:16:04.308737    1075 out.go:177] 💾  Downloading Kubernetes v1.32.0 preload ...
I0414 14:16:04.310583    1075 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0414 14:16:04.633675    1075 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4?checksum=md5:4da2ed9bc13e09e8e9b7cf53d01335db -> /home/joanna/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0414 14:17:35.041466    1075 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0414 14:17:35.041525    1075 preload.go:254] verifying checksum of /home/joanna/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0414 14:17:35.626311    1075 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0414 14:17:35.626573    1075 profile.go:143] Saving config to /home/joanna/.minikube/profiles/minikube/config.json ...
I0414 14:17:35.626592    1075 lock.go:35] WriteFile acquiring /home/joanna/.minikube/profiles/minikube/config.json: {Name:mk1679076f5aee76a9f8da66693fd67a0459299f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0414 14:17:39.787236    1075 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0414 14:17:39.787249    1075 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
I0414 14:17:50.393152    1075 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from cached tarball
I0414 14:17:50.393193    1075 cache.go:227] Successfully downloaded all kic artifacts
I0414 14:17:50.393240    1075 start.go:360] acquireMachinesLock for minikube: {Name:mkec5f9087abf2f78f5b78b6de96ae8b5c8acb55 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0414 14:17:50.393330    1075 start.go:364] duration metric: took 76.477µs to acquireMachinesLock for "minikube"
I0414 14:17:50.393345    1075 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/joanna:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0414 14:17:50.393395    1075 start.go:125] createHost starting for "" (driver="docker")
I0414 14:17:50.394964    1075 out.go:235] 🔥  Creating docker container (CPUs=2, Memory=3900MB) ...
I0414 14:17:50.395184    1075 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0414 14:17:50.395218    1075 client.go:168] LocalClient.Create starting
I0414 14:17:50.395289    1075 main.go:141] libmachine: Creating CA: /home/joanna/.minikube/certs/ca.pem
I0414 14:17:50.462652    1075 main.go:141] libmachine: Creating client certificate: /home/joanna/.minikube/certs/cert.pem
I0414 14:17:50.613071    1075 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0414 14:17:50.632672    1075 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0414 14:17:50.632728    1075 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0414 14:17:50.632740    1075 cli_runner.go:164] Run: docker network inspect minikube
W0414 14:17:50.647981    1075 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0414 14:17:50.647998    1075 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0414 14:17:50.648011    1075 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0414 14:17:50.648082    1075 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0414 14:17:50.662912    1075 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc017375580}
I0414 14:17:50.662936    1075 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0414 14:17:50.662966    1075 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0414 14:17:50.713720    1075 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0414 14:17:50.713747    1075 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0414 14:17:50.713800    1075 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0414 14:17:50.731093    1075 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0414 14:17:50.747619    1075 oci.go:103] Successfully created a docker volume minikube
I0414 14:17:50.747688    1075 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib
I0414 14:17:51.632608    1075 oci.go:107] Successfully prepared a docker volume minikube
I0414 14:17:51.632665    1075 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0414 14:17:51.632683    1075 kic.go:194] Starting extracting preloaded images to volume ...
I0414 14:17:51.632732    1075 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/joanna/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir
I0414 14:17:54.381194    1075 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/joanna/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir: (2.748427574s)
I0414 14:17:54.381216    1075 kic.go:203] duration metric: took 2.748528952s to extract preloaded images to volume ...
W0414 14:17:54.381490    1075 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I0414 14:17:54.381551    1075 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0414 14:17:54.545769    1075 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3900mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279
I0414 14:17:54.806657    1075 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0414 14:17:54.826596    1075 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0414 14:17:54.845874    1075 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0414 14:17:54.903705    1075 oci.go:144] the created container "minikube" has a running status.
I0414 14:17:54.903729    1075 kic.go:225] Creating ssh key for kic: /home/joanna/.minikube/machines/minikube/id_rsa...
I0414 14:17:55.171306    1075 kic_runner.go:191] docker (temp): /home/joanna/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0414 14:17:55.198785    1075 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0414 14:17:55.219110    1075 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0414 14:17:55.219122    1075 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0414 14:17:55.272342    1075 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0414 14:17:55.289989    1075 machine.go:93] provisionDockerMachine start ...
I0414 14:17:55.290084    1075 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 14:17:55.308546    1075 main.go:141] libmachine: Using SSH client type: native
I0414 14:17:55.308692    1075 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 51063 <nil> <nil>}
I0414 14:17:55.308696    1075 main.go:141] libmachine: About to run SSH command:
hostname
I0414 14:17:55.441240    1075 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0414 14:17:55.441257    1075 ubuntu.go:169] provisioning hostname "minikube"
I0414 14:17:55.441381    1075 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 14:17:55.461713    1075 main.go:141] libmachine: Using SSH client type: native
I0414 14:17:55.461872    1075 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 51063 <nil> <nil>}
I0414 14:17:55.461878    1075 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0414 14:17:55.588399    1075 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0414 14:17:55.588476    1075 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 14:17:55.607140    1075 main.go:141] libmachine: Using SSH client type: native
I0414 14:17:55.607305    1075 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 51063 <nil> <nil>}
I0414 14:17:55.607313    1075 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0414 14:17:55.741871    1075 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0414 14:17:55.741888    1075 ubuntu.go:175] set auth options {CertDir:/home/joanna/.minikube CaCertPath:/home/joanna/.minikube/certs/ca.pem CaPrivateKeyPath:/home/joanna/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/joanna/.minikube/machines/server.pem ServerKeyPath:/home/joanna/.minikube/machines/server-key.pem ClientKeyPath:/home/joanna/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/joanna/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/joanna/.minikube}
I0414 14:17:55.741911    1075 ubuntu.go:177] setting up certificates
I0414 14:17:55.741934    1075 provision.go:84] configureAuth start
I0414 14:17:55.742001    1075 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0414 14:17:55.762805    1075 provision.go:143] copyHostCerts
I0414 14:17:55.762851    1075 exec_runner.go:151] cp: /home/joanna/.minikube/certs/ca.pem --> /home/joanna/.minikube/ca.pem (1078 bytes)
I0414 14:17:55.762915    1075 exec_runner.go:151] cp: /home/joanna/.minikube/certs/cert.pem --> /home/joanna/.minikube/cert.pem (1123 bytes)
I0414 14:17:55.762943    1075 exec_runner.go:151] cp: /home/joanna/.minikube/certs/key.pem --> /home/joanna/.minikube/key.pem (1675 bytes)
I0414 14:17:55.762969    1075 provision.go:117] generating server cert: /home/joanna/.minikube/machines/server.pem ca-key=/home/joanna/.minikube/certs/ca.pem private-key=/home/joanna/.minikube/certs/ca-key.pem org=joanna.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0414 14:17:55.959734    1075 provision.go:177] copyRemoteCerts
I0414 14:17:55.959773    1075 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0414 14:17:55.959809    1075 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 14:17:55.981227    1075 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51063 SSHKeyPath:/home/joanna/.minikube/machines/minikube/id_rsa Username:docker}
I0414 14:17:56.092433    1075 ssh_runner.go:362] scp /home/joanna/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0414 14:17:56.108196    1075 ssh_runner.go:362] scp /home/joanna/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I0414 14:17:56.124537    1075 ssh_runner.go:362] scp /home/joanna/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0414 14:17:56.140507    1075 provision.go:87] duration metric: took 398.563324ms to configureAuth
I0414 14:17:56.140521    1075 ubuntu.go:193] setting minikube options for container-runtime
I0414 14:17:56.140659    1075 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0414 14:17:56.140703    1075 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 14:17:56.160454    1075 main.go:141] libmachine: Using SSH client type: native
I0414 14:17:56.160579    1075 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 51063 <nil> <nil>}
I0414 14:17:56.160585    1075 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0414 14:17:56.287181    1075 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0414 14:17:56.287202    1075 ubuntu.go:71] root file system type: overlay
I0414 14:17:56.287747    1075 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0414 14:17:56.287885    1075 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 14:17:56.307877    1075 main.go:141] libmachine: Using SSH client type: native
I0414 14:17:56.308005    1075 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 51063 <nil> <nil>}
I0414 14:17:56.308042    1075 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0414 14:17:56.439249    1075 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0414 14:17:56.439342    1075 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 14:17:56.458728    1075 main.go:141] libmachine: Using SSH client type: native
I0414 14:17:56.458909    1075 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 51063 <nil> <nil>}
I0414 14:17:56.458918    1075 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0414 14:17:58.663557    1075 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-12-17 15:44:19.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-04-14 12:17:56.430801242 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0414 14:17:58.663573    1075 machine.go:96] duration metric: took 3.373569993s to provisionDockerMachine
I0414 14:17:58.663582    1075 client.go:171] duration metric: took 8.268360076s to LocalClient.Create
I0414 14:17:58.663608    1075 start.go:167] duration metric: took 8.268425005s to libmachine.API.Create "minikube"
I0414 14:17:58.663614    1075 start.go:293] postStartSetup for "minikube" (driver="docker")
I0414 14:17:58.663621    1075 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0414 14:17:58.663667    1075 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0414 14:17:58.663701    1075 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 14:17:58.684518    1075 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51063 SSHKeyPath:/home/joanna/.minikube/machines/minikube/id_rsa Username:docker}
I0414 14:17:58.775397    1075 ssh_runner.go:195] Run: cat /etc/os-release
I0414 14:17:58.778161    1075 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0414 14:17:58.778173    1075 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0414 14:17:58.778186    1075 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0414 14:17:58.778192    1075 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0414 14:17:58.778198    1075 filesync.go:126] Scanning /home/joanna/.minikube/addons for local assets ...
I0414 14:17:58.778240    1075 filesync.go:126] Scanning /home/joanna/.minikube/files for local assets ...
I0414 14:17:58.778250    1075 start.go:296] duration metric: took 114.633104ms for postStartSetup
I0414 14:17:58.778449    1075 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0414 14:17:58.799603    1075 profile.go:143] Saving config to /home/joanna/.minikube/profiles/minikube/config.json ...
I0414 14:17:58.799766    1075 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0414 14:17:58.799794    1075 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 14:17:58.817121    1075 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51063 SSHKeyPath:/home/joanna/.minikube/machines/minikube/id_rsa Username:docker}
I0414 14:17:58.901590    1075 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0414 14:17:58.905017    1075 start.go:128] duration metric: took 8.511615672s to createHost
I0414 14:17:58.905025    1075 start.go:83] releasing machines lock for "minikube", held for 8.511690881s
I0414 14:17:58.905091    1075 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0414 14:17:58.924127    1075 ssh_runner.go:195] Run: cat /version.json
I0414 14:17:58.924161    1075 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 14:17:58.924249    1075 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0414 14:17:58.924285    1075 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 14:17:58.944156    1075 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51063 SSHKeyPath:/home/joanna/.minikube/machines/minikube/id_rsa Username:docker}
I0414 14:17:58.945303    1075 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51063 SSHKeyPath:/home/joanna/.minikube/machines/minikube/id_rsa Username:docker}
I0414 14:17:59.030585    1075 ssh_runner.go:195] Run: systemctl --version
I0414 14:17:59.262071    1075 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0414 14:17:59.265870    1075 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0414 14:17:59.282049    1075 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0414 14:17:59.282081    1075 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0414 14:17:59.299328    1075 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0414 14:17:59.299341    1075 start.go:495] detecting cgroup driver to use...
I0414 14:17:59.299364    1075 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0414 14:17:59.299440    1075 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0414 14:17:59.309254    1075 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0414 14:17:59.315618    1075 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0414 14:17:59.321663    1075 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0414 14:17:59.321684    1075 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0414 14:17:59.327543    1075 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0414 14:17:59.333748    1075 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0414 14:17:59.339719    1075 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0414 14:17:59.346496    1075 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0414 14:17:59.352306    1075 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0414 14:17:59.358938    1075 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0414 14:17:59.364977    1075 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0414 14:17:59.371182    1075 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0414 14:17:59.376248    1075 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0414 14:17:59.381874    1075 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0414 14:17:59.459279    1075 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0414 14:17:59.571180    1075 start.go:495] detecting cgroup driver to use...
I0414 14:17:59.571231    1075 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0414 14:17:59.571293    1075 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0414 14:17:59.579327    1075 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0414 14:17:59.579360    1075 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0414 14:17:59.586609    1075 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0414 14:17:59.596644    1075 ssh_runner.go:195] Run: which cri-dockerd
I0414 14:17:59.598963    1075 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0414 14:17:59.604517    1075 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0414 14:17:59.616071    1075 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0414 14:17:59.698560    1075 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0414 14:17:59.778445    1075 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0414 14:17:59.778547    1075 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0414 14:17:59.790415    1075 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0414 14:17:59.888362    1075 ssh_runner.go:195] Run: sudo systemctl restart docker
I0414 14:18:02.059512    1075 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.468777928s)
I0414 14:18:02.059575    1075 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0414 14:18:02.067483    1075 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0414 14:18:02.075113    1075 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0414 14:18:02.162231    1075 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0414 14:18:02.251160    1075 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0414 14:18:02.321259    1075 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0414 14:18:02.331018    1075 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0414 14:18:02.338251    1075 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0414 14:18:02.422015    1075 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0414 14:18:02.475509    1075 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0414 14:18:02.475557    1075 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0414 14:18:02.478114    1075 start.go:563] Will wait 60s for crictl version
I0414 14:18:02.478144    1075 ssh_runner.go:195] Run: which crictl
I0414 14:18:02.481042    1075 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0414 14:18:02.510849    1075 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0414 14:18:02.510919    1075 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0414 14:18:02.529568    1075 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0414 14:18:02.549501    1075 out.go:235] 🐳  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0414 14:18:02.549657    1075 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0414 14:18:02.565715    1075 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0414 14:18:02.568591    1075 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0414 14:18:02.576018    1075 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0414 14:18:02.593838    1075 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/joanna:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0414 14:18:02.593922    1075 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0414 14:18:02.593963    1075 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0414 14:18:02.609191    1075 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0414 14:18:02.609203    1075 docker.go:619] Images already preloaded, skipping extraction
I0414 14:18:02.609250    1075 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0414 14:18:02.624476    1075 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0414 14:18:02.624489    1075 cache_images.go:84] Images are preloaded, skipping loading
I0414 14:18:02.624497    1075 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0414 14:18:02.624571    1075 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0414 14:18:02.624613    1075 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0414 14:18:02.658096    1075 cni.go:84] Creating CNI manager for ""
I0414 14:18:02.658110    1075 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0414 14:18:02.658118    1075 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0414 14:18:02.658133    1075 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0414 14:18:02.658243    1075 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0414 14:18:02.658309    1075 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0414 14:18:02.664706    1075 binaries.go:44] Found k8s binaries, skipping transfer
I0414 14:18:02.664741    1075 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0414 14:18:02.670620    1075 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0414 14:18:02.682014    1075 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0414 14:18:02.693263    1075 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0414 14:18:02.705054    1075 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0414 14:18:02.707589    1075 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0414 14:18:02.715247    1075 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0414 14:18:02.798419    1075 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0414 14:18:02.808915    1075 certs.go:68] Setting up /home/joanna/.minikube/profiles/minikube for IP: 192.168.49.2
I0414 14:18:02.808940    1075 certs.go:194] generating shared ca certs ...
I0414 14:18:02.808954    1075 certs.go:226] acquiring lock for ca certs: {Name:mkd2b05f2d4375047a9f96e02e3d5c03782c746f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0414 14:18:02.809070    1075 certs.go:240] generating "minikubeCA" ca cert: /home/joanna/.minikube/ca.key
I0414 14:18:02.930240    1075 crypto.go:156] Writing cert to /home/joanna/.minikube/ca.crt ...
I0414 14:18:02.930254    1075 lock.go:35] WriteFile acquiring /home/joanna/.minikube/ca.crt: {Name:mk84f0b3f190a3b3f454cf2850997c3db14be8d8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0414 14:18:02.930393    1075 crypto.go:164] Writing key to /home/joanna/.minikube/ca.key ...
I0414 14:18:02.930398    1075 lock.go:35] WriteFile acquiring /home/joanna/.minikube/ca.key: {Name:mk72038bee251df551af4a5b68ee962e430b3b4a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0414 14:18:02.930460    1075 certs.go:240] generating "proxyClientCA" ca cert: /home/joanna/.minikube/proxy-client-ca.key
I0414 14:18:03.002616    1075 crypto.go:156] Writing cert to /home/joanna/.minikube/proxy-client-ca.crt ...
I0414 14:18:03.002628    1075 lock.go:35] WriteFile acquiring /home/joanna/.minikube/proxy-client-ca.crt: {Name:mkf0bf1515d79aa07369bee1b41cf31937640d33 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0414 14:18:03.002753    1075 crypto.go:164] Writing key to /home/joanna/.minikube/proxy-client-ca.key ...
I0414 14:18:03.002757    1075 lock.go:35] WriteFile acquiring /home/joanna/.minikube/proxy-client-ca.key: {Name:mkdb12f04abe34bec2a54f6cc181f009d08f7142 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0414 14:18:03.002811    1075 certs.go:256] generating profile certs ...
I0414 14:18:03.002851    1075 certs.go:363] generating signed profile cert for "minikube-user": /home/joanna/.minikube/profiles/minikube/client.key
I0414 14:18:03.002863    1075 crypto.go:68] Generating cert /home/joanna/.minikube/profiles/minikube/client.crt with IP's: []
I0414 14:18:03.104297    1075 crypto.go:156] Writing cert to /home/joanna/.minikube/profiles/minikube/client.crt ...
I0414 14:18:03.104313    1075 lock.go:35] WriteFile acquiring /home/joanna/.minikube/profiles/minikube/client.crt: {Name:mk53e271233bcb98e9cc87c18f6088580077a458 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0414 14:18:03.104698    1075 crypto.go:164] Writing key to /home/joanna/.minikube/profiles/minikube/client.key ...
I0414 14:18:03.104707    1075 lock.go:35] WriteFile acquiring /home/joanna/.minikube/profiles/minikube/client.key: {Name:mk57997a41bada92f6865b7114840cd714b51f38 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0414 14:18:03.104797    1075 certs.go:363] generating signed profile cert for "minikube": /home/joanna/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0414 14:18:03.104817    1075 crypto.go:68] Generating cert /home/joanna/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0414 14:18:03.220772    1075 crypto.go:156] Writing cert to /home/joanna/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0414 14:18:03.220786    1075 lock.go:35] WriteFile acquiring /home/joanna/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk6ae5ba08347e5d0dae4d104f5e8700c32969a6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0414 14:18:03.220914    1075 crypto.go:164] Writing key to /home/joanna/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0414 14:18:03.220919    1075 lock.go:35] WriteFile acquiring /home/joanna/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk1fa04893773b90ce16302c2dcf2576724ea6af Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0414 14:18:03.220977    1075 certs.go:381] copying /home/joanna/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/joanna/.minikube/profiles/minikube/apiserver.crt
I0414 14:18:03.221052    1075 certs.go:385] copying /home/joanna/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/joanna/.minikube/profiles/minikube/apiserver.key
I0414 14:18:03.221083    1075 certs.go:363] generating signed profile cert for "aggregator": /home/joanna/.minikube/profiles/minikube/proxy-client.key
I0414 14:18:03.221094    1075 crypto.go:68] Generating cert /home/joanna/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0414 14:18:03.350403    1075 crypto.go:156] Writing cert to /home/joanna/.minikube/profiles/minikube/proxy-client.crt ...
I0414 14:18:03.350417    1075 lock.go:35] WriteFile acquiring /home/joanna/.minikube/profiles/minikube/proxy-client.crt: {Name:mk108c91214837b3b567c77c2dbb4c1ca55de9c0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0414 14:18:03.350541    1075 crypto.go:164] Writing key to /home/joanna/.minikube/profiles/minikube/proxy-client.key ...
I0414 14:18:03.350546    1075 lock.go:35] WriteFile acquiring /home/joanna/.minikube/profiles/minikube/proxy-client.key: {Name:mkdf3426711c3433350782fe8201210284aecc85 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0414 14:18:03.350680    1075 certs.go:484] found cert: /home/joanna/.minikube/certs/ca-key.pem (1675 bytes)
I0414 14:18:03.350699    1075 certs.go:484] found cert: /home/joanna/.minikube/certs/ca.pem (1078 bytes)
I0414 14:18:03.350712    1075 certs.go:484] found cert: /home/joanna/.minikube/certs/cert.pem (1123 bytes)
I0414 14:18:03.350724    1075 certs.go:484] found cert: /home/joanna/.minikube/certs/key.pem (1675 bytes)
I0414 14:18:03.351120    1075 ssh_runner.go:362] scp /home/joanna/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0414 14:18:03.367094    1075 ssh_runner.go:362] scp /home/joanna/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0414 14:18:03.382085    1075 ssh_runner.go:362] scp /home/joanna/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0414 14:18:03.396725    1075 ssh_runner.go:362] scp /home/joanna/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0414 14:18:03.412648    1075 ssh_runner.go:362] scp /home/joanna/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0414 14:18:03.427930    1075 ssh_runner.go:362] scp /home/joanna/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0414 14:18:03.442454    1075 ssh_runner.go:362] scp /home/joanna/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0414 14:18:03.456900    1075 ssh_runner.go:362] scp /home/joanna/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0414 14:18:03.471123    1075 ssh_runner.go:362] scp /home/joanna/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0414 14:18:03.486361    1075 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0414 14:18:03.497813    1075 ssh_runner.go:195] Run: openssl version
I0414 14:18:03.501390    1075 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0414 14:18:03.507741    1075 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0414 14:18:03.510178    1075 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Apr 14 12:18 /usr/share/ca-certificates/minikubeCA.pem
I0414 14:18:03.510252    1075 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0414 14:18:03.514603    1075 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0414 14:18:03.520388    1075 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0414 14:18:03.522880    1075 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0414 14:18:03.522905    1075 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/joanna:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0414 14:18:03.522968    1075 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0414 14:18:03.539066    1075 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0414 14:18:03.544745    1075 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0414 14:18:03.550413    1075 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0414 14:18:03.550431    1075 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0414 14:18:03.555713    1075 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0414 14:18:03.555717    1075 kubeadm.go:157] found existing configuration files:

I0414 14:18:03.555736    1075 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0414 14:18:03.560772    1075 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0414 14:18:03.560915    1075 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0414 14:18:03.566209    1075 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0414 14:18:03.571722    1075 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0414 14:18:03.571740    1075 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0414 14:18:03.577138    1075 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0414 14:18:03.582588    1075 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0414 14:18:03.582605    1075 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0414 14:18:03.587693    1075 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0414 14:18:03.592936    1075 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0414 14:18:03.592952    1075 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0414 14:18:03.598368    1075 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0414 14:18:03.626726    1075 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0414 14:18:03.626760    1075 kubeadm.go:310] [preflight] Running pre-flight checks
I0414 14:18:03.687119    1075 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0414 14:18:03.687189    1075 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0414 14:18:03.687260    1075 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0414 14:18:03.695805    1075 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0414 14:18:03.704983    1075 out.go:235]     ▪ Generating certificates and keys ...
I0414 14:18:03.705128    1075 kubeadm.go:310] [certs] Using existing ca certificate authority
I0414 14:18:03.705166    1075 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0414 14:18:03.837717    1075 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0414 14:18:03.913197    1075 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0414 14:18:04.016515    1075 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0414 14:18:04.156985    1075 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0414 14:18:04.361965    1075 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0414 14:18:04.362034    1075 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0414 14:18:04.518241    1075 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0414 14:18:04.518310    1075 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0414 14:18:04.636901    1075 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0414 14:18:04.793256    1075 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0414 14:18:04.844634    1075 kubeadm.go:310] [certs] Generating "sa" key and public key
I0414 14:18:04.844673    1075 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0414 14:18:05.091785    1075 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0414 14:18:05.244825    1075 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0414 14:18:05.301072    1075 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0414 14:18:05.439198    1075 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0414 14:18:05.564608    1075 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0414 14:18:05.565047    1075 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0414 14:18:05.566706    1075 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0414 14:18:05.569868    1075 out.go:235]     ▪ Booting up control plane ...
I0414 14:18:05.570006    1075 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0414 14:18:05.570051    1075 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0414 14:18:05.570088    1075 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0414 14:18:05.574810    1075 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0414 14:18:05.578685    1075 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0414 14:18:05.578714    1075 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0414 14:18:05.654976    1075 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0414 14:18:05.655072    1075 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0414 14:18:06.157469    1075 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 502.387344ms
I0414 14:18:06.157531    1075 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0414 14:18:10.160529    1075 kubeadm.go:310] [api-check] The API server is healthy after 4.003189071s
I0414 14:18:10.170666    1075 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0414 14:18:10.181539    1075 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0414 14:18:10.195712    1075 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0414 14:18:10.195864    1075 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0414 14:18:10.200916    1075 kubeadm.go:310] [bootstrap-token] Using token: s0rzvr.r1k2x0k09bum41gs
I0414 14:18:10.202310    1075 out.go:235]     ▪ Configuring RBAC rules ...
I0414 14:18:10.202433    1075 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0414 14:18:10.204191    1075 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0414 14:18:10.208449    1075 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0414 14:18:10.212222    1075 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0414 14:18:10.214094    1075 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0414 14:18:10.216034    1075 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0414 14:18:10.564972    1075 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0414 14:18:11.048306    1075 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0414 14:18:11.566615    1075 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0414 14:18:11.567635    1075 kubeadm.go:310] 
I0414 14:18:11.567701    1075 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0414 14:18:11.567705    1075 kubeadm.go:310] 
I0414 14:18:11.567780    1075 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0414 14:18:11.567783    1075 kubeadm.go:310] 
I0414 14:18:11.567812    1075 kubeadm.go:310]   mkdir -p $HOME/.kube
I0414 14:18:11.567848    1075 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0414 14:18:11.567889    1075 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0414 14:18:11.567891    1075 kubeadm.go:310] 
I0414 14:18:11.567944    1075 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0414 14:18:11.567949    1075 kubeadm.go:310] 
I0414 14:18:11.567986    1075 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0414 14:18:11.567988    1075 kubeadm.go:310] 
I0414 14:18:11.568018    1075 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0414 14:18:11.568060    1075 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0414 14:18:11.568099    1075 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0414 14:18:11.568101    1075 kubeadm.go:310] 
I0414 14:18:11.568176    1075 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0414 14:18:11.568240    1075 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0414 14:18:11.568243    1075 kubeadm.go:310] 
I0414 14:18:11.568315    1075 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token s0rzvr.r1k2x0k09bum41gs \
I0414 14:18:11.568416    1075 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:9e070ee963db9024b6e645c39714bd0c555741871d8a3178579209a50f2d1184 \
I0414 14:18:11.568433    1075 kubeadm.go:310] 	--control-plane 
I0414 14:18:11.568436    1075 kubeadm.go:310] 
I0414 14:18:11.568505    1075 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0414 14:18:11.568508    1075 kubeadm.go:310] 
I0414 14:18:11.568554    1075 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token s0rzvr.r1k2x0k09bum41gs \
I0414 14:18:11.568630    1075 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:9e070ee963db9024b6e645c39714bd0c555741871d8a3178579209a50f2d1184 
I0414 14:18:11.570399    1075 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0414 14:18:11.570464    1075 kubeadm.go:310] 	[WARNING SystemVerification]: cgroups v1 support is in maintenance mode, please migrate to cgroups v2
I0414 14:18:11.570592    1075 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0414 14:18:11.570606    1075 cni.go:84] Creating CNI manager for ""
I0414 14:18:11.570617    1075 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0414 14:18:11.572682    1075 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0414 14:18:11.574341    1075 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0414 14:18:11.580610    1075 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0414 14:18:11.593324    1075 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0414 14:18:11.593394    1075 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0414 14:18:11.593446    1075 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_04_14T14_18_11_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0414 14:18:11.660469    1075 kubeadm.go:1113] duration metric: took 67.138244ms to wait for elevateKubeSystemPrivileges
I0414 14:18:11.660545    1075 ops.go:34] apiserver oom_adj: -16
I0414 14:18:11.660572    1075 kubeadm.go:394] duration metric: took 8.137669545s to StartCluster
I0414 14:18:11.660589    1075 settings.go:142] acquiring lock: {Name:mk8a6fe5935078945daaab02e45606e9157ed9c9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0414 14:18:11.660694    1075 settings.go:150] Updating kubeconfig:  /home/joanna/.kube/config
I0414 14:18:11.661098    1075 lock.go:35] WriteFile acquiring /home/joanna/.kube/config: {Name:mkd8242b3a6ade7eb968d2d1362156add4cb5ad3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0414 14:18:11.661260    1075 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0414 14:18:11.661259    1075 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0414 14:18:11.661333    1075 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0414 14:18:11.661402    1075 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0414 14:18:11.661415    1075 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0414 14:18:11.661421    1075 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0414 14:18:11.661425    1075 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0414 14:18:11.661435    1075 host.go:66] Checking if "minikube" exists ...
I0414 14:18:11.661435    1075 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0414 14:18:11.661646    1075 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0414 14:18:11.661693    1075 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0414 14:18:11.663116    1075 out.go:177] 🔎  Verifying Kubernetes components...
I0414 14:18:11.665289    1075 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0414 14:18:11.683457    1075 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0414 14:18:11.683481    1075 host.go:66] Checking if "minikube" exists ...
I0414 14:18:11.683724    1075 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0414 14:18:11.685153    1075 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0414 14:18:11.686557    1075 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0414 14:18:11.686566    1075 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0414 14:18:11.686603    1075 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 14:18:11.706172    1075 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0414 14:18:11.706187    1075 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0414 14:18:11.706248    1075 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 14:18:11.707659    1075 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51063 SSHKeyPath:/home/joanna/.minikube/machines/minikube/id_rsa Username:docker}
I0414 14:18:11.723942    1075 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0414 14:18:11.728077    1075 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51063 SSHKeyPath:/home/joanna/.minikube/machines/minikube/id_rsa Username:docker}
I0414 14:18:11.756144    1075 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0414 14:18:11.834663    1075 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0414 14:18:11.834850    1075 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0414 14:18:11.951888    1075 start.go:971] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I0414 14:18:11.951978    1075 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0414 14:18:11.971472    1075 api_server.go:52] waiting for apiserver process to appear ...
I0414 14:18:11.971506    1075 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0414 14:18:12.157899    1075 api_server.go:72] duration metric: took 496.620867ms to wait for apiserver process to appear ...
I0414 14:18:12.157912    1075 api_server.go:88] waiting for apiserver healthz status ...
I0414 14:18:12.157929    1075 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51062/healthz ...
I0414 14:18:12.161367    1075 api_server.go:279] https://127.0.0.1:51062/healthz returned 200:
ok
I0414 14:18:12.161980    1075 api_server.go:141] control plane version: v1.32.0
I0414 14:18:12.161991    1075 api_server.go:131] duration metric: took 4.07446ms to wait for apiserver health ...
I0414 14:18:12.161996    1075 system_pods.go:43] waiting for kube-system pods to appear ...
I0414 14:18:12.162813    1075 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0414 14:18:12.164071    1075 addons.go:514] duration metric: took 502.743788ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0414 14:18:12.165631    1075 system_pods.go:59] 5 kube-system pods found
I0414 14:18:12.165641    1075 system_pods.go:61] "etcd-minikube" [a2c84bb0-8aae-4289-a675-af84bc03dc4a] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0414 14:18:12.165645    1075 system_pods.go:61] "kube-apiserver-minikube" [5f6444ea-3498-4348-816c-ec63e73cbc9c] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0414 14:18:12.165648    1075 system_pods.go:61] "kube-controller-manager-minikube" [169428a7-f0af-4efd-becc-54f2e1647874] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0414 14:18:12.165651    1075 system_pods.go:61] "kube-scheduler-minikube" [2d1b0069-2f7f-46da-b3eb-21c24fc5de21] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0414 14:18:12.165653    1075 system_pods.go:61] "storage-provisioner" [28d2a906-4254-4e91-9602-97945cc9b22c] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0414 14:18:12.165656    1075 system_pods.go:74] duration metric: took 3.65739ms to wait for pod list to return data ...
I0414 14:18:12.165664    1075 kubeadm.go:582] duration metric: took 504.390651ms to wait for: map[apiserver:true system_pods:true]
I0414 14:18:12.165671    1075 node_conditions.go:102] verifying NodePressure condition ...
I0414 14:18:12.167678    1075 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0414 14:18:12.167686    1075 node_conditions.go:123] node cpu capacity is 16
I0414 14:18:12.167697    1075 node_conditions.go:105] duration metric: took 2.023553ms to run NodePressure ...
I0414 14:18:12.167707    1075 start.go:241] waiting for startup goroutines ...
I0414 14:18:12.454581    1075 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0414 14:18:12.454613    1075 start.go:246] waiting for cluster config update ...
I0414 14:18:12.454622    1075 start.go:255] writing updated cluster config ...
I0414 14:18:12.454836    1075 ssh_runner.go:195] Run: rm -f paused
I0414 14:18:12.535770    1075 start.go:600] kubectl: 1.32.3, cluster: 1.32.0 (minor skew: 0)
I0414 14:18:12.538010    1075 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Apr 14 12:44:20 minikube dockerd[1410]: time="2025-04-14T12:44:20.192057193Z" level=info msg="ignoring event" container=a8d30d515c92bdd1d95330509d395b5a61d54dbead7af59e752f86e348bfb546 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 14 12:44:56 minikube dockerd[1410]: time="2025-04-14T12:44:56.414884891Z" level=info msg="ignoring event" container=bdc6321873a1c152c3792efc0ccd7813551730b56718a32bd8d37ca9d4a725a0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 14 12:44:56 minikube dockerd[1410]: time="2025-04-14T12:44:56.415026295Z" level=info msg="ignoring event" container=4d51b6cf460f2d49ee41c05b67dbbd90ff9217adca87dd69c858d905cb1b3c75 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 14 12:44:57 minikube dockerd[1410]: time="2025-04-14T12:44:57.002989035Z" level=info msg="ignoring event" container=e7eabe2105b814807c73df994075671119040ab045571bf90cf51a37f994d03f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 14 12:44:57 minikube cri-dockerd[1689]: time="2025-04-14T12:44:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-node-64bd67f586-h49g6_default\": unexpected command output nsenter: cannot open /proc/9422/ns/net: No such file or directory\n with error: exit status 1"
Apr 14 12:44:57 minikube dockerd[1410]: time="2025-04-14T12:44:57.157208352Z" level=info msg="ignoring event" container=ce4eb63182f46a0808e1e43f7527bfef542149d82735b7f02371f9f2531d55f1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 14 12:45:12 minikube cri-dockerd[1689]: time="2025-04-14T12:45:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/375f0d79925b2bac13865ea218e96c72d0384b16420c4b6e39b52922f966d8b1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 14 12:45:18 minikube cri-dockerd[1689]: time="2025-04-14T12:45:18Z" level=info msg="Stop pulling image nginxdemos/hello:latest: Status: Downloaded newer image for nginxdemos/hello:latest"
Apr 14 12:47:11 minikube cri-dockerd[1689]: time="2025-04-14T12:47:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6c7af8ddd56faf0e5c728f2f0e495b4ae580e8a6e8de613ff4529a1ab4aa7725/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 14 12:47:13 minikube cri-dockerd[1689]: time="2025-04-14T12:47:13Z" level=info msg="Stop pulling image nginxdemos/hello:latest: Status: Image is up to date for nginxdemos/hello:latest"
Apr 14 12:54:00 minikube cri-dockerd[1689]: time="2025-04-14T12:54:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/43cd81b025a77819f617acc3c7ae0f47f9c3e707e98c691e36d64ece14f18fc4/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 14 12:54:00 minikube cri-dockerd[1689]: time="2025-04-14T12:54:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4de40837517924ac038cc34c42f170b3ea308f67b9ebbcd9c9917978b4e91811/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 14 12:54:01 minikube cri-dockerd[1689]: time="2025-04-14T12:54:01Z" level=info msg="Stop pulling image nginxdemos/hello:latest: Status: Image is up to date for nginxdemos/hello:latest"
Apr 14 12:54:02 minikube cri-dockerd[1689]: time="2025-04-14T12:54:02Z" level=info msg="Stop pulling image nginxdemos/hello:latest: Status: Image is up to date for nginxdemos/hello:latest"
Apr 14 12:55:57 minikube cri-dockerd[1689]: time="2025-04-14T12:55:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cc4b910d4c382d33525f136f8bbe36e2cbaddbcf41091c10ccec56ebe0fd66e2/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 14 12:55:57 minikube cri-dockerd[1689]: time="2025-04-14T12:55:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5a4f4ed5e964493e222916ba36b9d6f5bf053e5f58403f063d4e69b91b571e55/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 14 12:55:57 minikube dockerd[1410]: time="2025-04-14T12:55:57.772945774Z" level=warning msg="reference for unknown type: " digest="sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c" remote="docker.io/kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Apr 14 12:56:02 minikube cri-dockerd[1689]: time="2025-04-14T12:56:02Z" level=info msg="Stop pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: Status: Downloaded newer image for kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
Apr 14 12:56:02 minikube dockerd[1410]: time="2025-04-14T12:56:02.560089062Z" level=warning msg="reference for unknown type: " digest="sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93" remote="docker.io/kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Apr 14 12:56:13 minikube cri-dockerd[1689]: time="2025-04-14T12:56:13Z" level=info msg="Pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: ee3247c7e545: Downloading [==============================>                    ]  45.78MB/75.78MB"
Apr 14 12:56:20 minikube cri-dockerd[1689]: time="2025-04-14T12:56:20Z" level=info msg="Stop pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: Status: Downloaded newer image for kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
Apr 14 12:59:40 minikube cri-dockerd[1689]: time="2025-04-14T12:59:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fefdadc9380a34bc263b472933c5eae687f3bc99ab96d4ca51435c7c562cb072/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 14 12:59:42 minikube dockerd[1410]: time="2025-04-14T12:59:42.911715399Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 12:59:42 minikube dockerd[1410]: time="2025-04-14T12:59:42.911765240Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 12:59:56 minikube dockerd[1410]: time="2025-04-14T12:59:56.248275972Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 12:59:56 minikube dockerd[1410]: time="2025-04-14T12:59:56.248384364Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 13:00:24 minikube dockerd[1410]: time="2025-04-14T13:00:24.016612176Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 13:00:24 minikube dockerd[1410]: time="2025-04-14T13:00:24.016766846Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 13:00:51 minikube dockerd[1410]: time="2025-04-14T13:00:51.201597368Z" level=info msg="ignoring event" container=fefdadc9380a34bc263b472933c5eae687f3bc99ab96d4ca51435c7c562cb072 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 14 13:01:55 minikube cri-dockerd[1689]: time="2025-04-14T13:01:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6376e4c9ceade7efa38c827c19fd7466723285e339979f4bcb698e6a8e2d2c2b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 14 13:01:57 minikube dockerd[1410]: time="2025-04-14T13:01:57.664343763Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 13:01:57 minikube dockerd[1410]: time="2025-04-14T13:01:57.664403041Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 13:02:12 minikube dockerd[1410]: time="2025-04-14T13:02:12.057529024Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 13:02:12 minikube dockerd[1410]: time="2025-04-14T13:02:12.057657520Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 13:02:40 minikube dockerd[1410]: time="2025-04-14T13:02:40.190371623Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 13:02:40 minikube dockerd[1410]: time="2025-04-14T13:02:40.190476341Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 13:03:33 minikube dockerd[1410]: time="2025-04-14T13:03:33.820329524Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 13:03:33 minikube dockerd[1410]: time="2025-04-14T13:03:33.820477273Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 13:03:59 minikube dockerd[1410]: time="2025-04-14T13:03:59.808625648Z" level=info msg="ignoring event" container=6376e4c9ceade7efa38c827c19fd7466723285e339979f4bcb698e6a8e2d2c2b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 14 13:04:03 minikube cri-dockerd[1689]: time="2025-04-14T13:04:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/234a660c43bc296c9b86cd6075652adbfff8924e5eded939033d6fc9e6fa07c0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 14 13:04:03 minikube dockerd[1410]: time="2025-04-14T13:04:03.328830201Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 13:04:03 minikube dockerd[1410]: time="2025-04-14T13:04:03.328883750Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 13:04:04 minikube dockerd[1410]: time="2025-04-14T13:04:04.131846778Z" level=info msg="ignoring event" container=234a660c43bc296c9b86cd6075652adbfff8924e5eded939033d6fc9e6fa07c0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 14 13:04:04 minikube cri-dockerd[1689]: time="2025-04-14T13:04:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/54da2766b5f5c1f8823ec9e31c533e914efc9fdd6bbbaf59fe5af3567c4fd3db/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 14 13:04:17 minikube dockerd[1410]: time="2025-04-14T13:04:17.457436248Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 13:04:17 minikube dockerd[1410]: time="2025-04-14T13:04:17.457521978Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 13:04:41 minikube dockerd[1410]: time="2025-04-14T13:04:41.875836927Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 13:04:41 minikube dockerd[1410]: time="2025-04-14T13:04:41.875959980Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 13:05:30 minikube dockerd[1410]: time="2025-04-14T13:05:30.448427869Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 13:05:30 minikube dockerd[1410]: time="2025-04-14T13:05:30.448542178Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 13:06:59 minikube dockerd[1410]: time="2025-04-14T13:06:59.569036392Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 13:06:59 minikube dockerd[1410]: time="2025-04-14T13:06:59.569088400Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 13:07:00 minikube dockerd[1410]: time="2025-04-14T13:07:00.721618037Z" level=info msg="ignoring event" container=54da2766b5f5c1f8823ec9e31c533e914efc9fdd6bbbaf59fe5af3567c4fd3db module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 14 13:07:04 minikube cri-dockerd[1689]: time="2025-04-14T13:07:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e19d226d23a6411502420f93be947ca67a158ec5b524c7b621953b4f99153dbe/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 14 13:07:06 minikube dockerd[1410]: time="2025-04-14T13:07:06.166103334Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 13:07:06 minikube dockerd[1410]: time="2025-04-14T13:07:06.166152211Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 13:07:21 minikube dockerd[1410]: time="2025-04-14T13:07:21.030181488Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 13:07:21 minikube dockerd[1410]: time="2025-04-14T13:07:21.030290803Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 13:07:46 minikube dockerd[1410]: time="2025-04-14T13:07:46.888172228Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 13:07:46 minikube dockerd[1410]: time="2025-04-14T13:07:46.888227088Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE                                                                                                  CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
cbfda574a2b17       kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93         11 minutes ago      Running             kubernetes-dashboard        0                   5a4f4ed5e9644       kubernetes-dashboard-7779f9b69b-dkqbz
ac6954a340530       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   11 minutes ago      Running             dashboard-metrics-scraper   0                   cc4b910d4c382       dashboard-metrics-scraper-5d59dccf9b-rckfp
defc5bad13f73       nginxdemos/hello@sha256:2293656951429d36b788d1285a97bd2a862428759802444450257760790423f6               13 minutes ago      Running             nginx-hello                 0                   4de4083751792       nginx-hello-b84ccf4b9-tq6jn
af6dd6ad7a3a1       nginxdemos/hello@sha256:2293656951429d36b788d1285a97bd2a862428759802444450257760790423f6               13 minutes ago      Running             nginx-hello                 0                   43cd81b025a77       nginx-hello-b84ccf4b9-kzct9
23bce6bf85db8       nginxdemos/hello@sha256:2293656951429d36b788d1285a97bd2a862428759802444450257760790423f6               20 minutes ago      Running             nginx-hello                 0                   6c7af8ddd56fa       nginx-hello-b84ccf4b9-wjngw
b50b8dd68b33c       nginxdemos/hello@sha256:2293656951429d36b788d1285a97bd2a862428759802444450257760790423f6               22 minutes ago      Running             hello                       0                   375f0d79925b2       hello-node-78d8c77f85-z9xcf
0b976504b3f6d       6e38f40d628db                                                                                          49 minutes ago      Running             storage-provisioner         1                   6442320b8dc76       storage-provisioner
884ce9f3cfc33       6e38f40d628db                                                                                          49 minutes ago      Exited              storage-provisioner         0                   6442320b8dc76       storage-provisioner
5b2f71a9b3500       040f9f8aac8cd                                                                                          49 minutes ago      Running             kube-proxy                  0                   deab394f40099       kube-proxy-k7bnh
99d5a8b1ed370       c69fa2e9cbf5f                                                                                          49 minutes ago      Running             coredns                     0                   533f238112098       coredns-668d6bf9bc-jggcn
847ace5575140       8cab3d2a8bd0f                                                                                          49 minutes ago      Running             kube-controller-manager     0                   277fb4402a2f4       kube-controller-manager-minikube
a01d4435ce674       c2e17b8d0f4a3                                                                                          49 minutes ago      Running             kube-apiserver              0                   297bd9347199b       kube-apiserver-minikube
180dcbd0e0b5f       a389e107f4ff1                                                                                          49 minutes ago      Running             kube-scheduler              0                   b96d5737f9c8f       kube-scheduler-minikube
7f3b8de2903d4       a9e7e6b294baf                                                                                          49 minutes ago      Running             etcd                        0                   0988c55829f40       etcd-minikube


==> coredns [99d5a8b1ed37] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:54458 - 55349 "HINFO IN 7158307041031727385.2646285232293983577. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.026855803s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1767930149]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (14-Apr-2025 12:18:16.864) (total time: 22017ms):
Trace[1767930149]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 22017ms (12:18:37.474)
Trace[1767930149]: [22.017442701s] [22.017442701s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[756354968]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (14-Apr-2025 12:18:16.864) (total time: 22017ms):
Trace[756354968]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 22017ms (12:18:37.474)
Trace[756354968]: [22.017596953s] [22.017596953s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[24206575]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (14-Apr-2025 12:18:16.864) (total time: 22017ms):
Trace[24206575]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 22017ms (12:18:37.474)
Trace[24206575]: [22.017631847s] [22.017631847s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_04_14T14_18_11_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 14 Apr 2025 12:18:08 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 14 Apr 2025 13:07:44 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 14 Apr 2025 13:07:32 +0000   Mon, 14 Apr 2025 12:18:07 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 14 Apr 2025 13:07:32 +0000   Mon, 14 Apr 2025 12:18:07 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 14 Apr 2025 13:07:32 +0000   Mon, 14 Apr 2025 12:18:07 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 14 Apr 2025 13:07:32 +0000   Mon, 14 Apr 2025 12:18:08 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16240976Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16240976Ki
  pods:               110
System Info:
  Machine ID:                 e334f56b379049bb8c258778b2e05304
  System UUID:                e334f56b379049bb8c258778b2e05304
  Boot ID:                    360b57db-7312-4e4f-9d5f-2dc0e2842d05
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     hello-node-78d8c77f85-z9xcf                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m
  default                     joanna-web-6b97746c44-hs4pc                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         48s
  default                     nginx-hello-b84ccf4b9-kzct9                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m
  default                     nginx-hello-b84ccf4b9-tq6jn                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m
  default                     nginx-hello-b84ccf4b9-wjngw                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m
  kube-system                 coredns-668d6bf9bc-jggcn                      100m (0%)     0 (0%)      70Mi (0%)        170Mi (1%)     49m
  kube-system                 etcd-minikube                                 100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         49m
  kube-system                 kube-apiserver-minikube                       250m (1%)     0 (0%)      0 (0%)           0 (0%)         49m
  kube-system                 kube-controller-manager-minikube              200m (1%)     0 (0%)      0 (0%)           0 (0%)         49m
  kube-system                 kube-proxy-k7bnh                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         49m
  kube-system                 kube-scheduler-minikube                       100m (0%)     0 (0%)      0 (0%)           0 (0%)         49m
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         49m
  kubernetes-dashboard        dashboard-metrics-scraper-5d59dccf9b-rckfp    0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m
  kubernetes-dashboard        kubernetes-dashboard-7779f9b69b-dkqbz         0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (4%)   0 (0%)
  memory             170Mi (1%)  170Mi (1%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           49m                kube-proxy       
  Normal   NodeHasSufficientMemory            49m (x8 over 49m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              49m (x8 over 49m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               49m (x7 over 49m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            49m                kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                           49m                kubelet          Starting kubelet.
  Warning  CgroupV1                           49m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Warning  PossibleMemoryBackedVolumesOnDisk  49m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   NodeAllocatableEnforced            49m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            49m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              49m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               49m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     49m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000295] FS-Cache: O-key=[10] '34323934393337333937'
[  +0.000178] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000235] FS-Cache: N-cookie d=00000000a61833f7{9P.session} n=00000000f27fa07c
[  +0.000314] FS-Cache: N-key=[10] '34323934393337333937'
[  +0.001078] FS-Cache: Duplicate cookie detected
[  +0.000235] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.000224] FS-Cache: O-cookie d=00000000a61833f7{9P.session} n=00000000cdf7b42a
[  +0.000266] FS-Cache: O-key=[10] '34323934393337333937'
[  +0.000179] FS-Cache: N-cookie c=00000007 [p=00000002 fl=2 nc=0 na=1]
[  +0.000220] FS-Cache: N-cookie d=00000000a61833f7{9P.session} n=000000003dde1767
[  +0.000287] FS-Cache: N-key=[10] '34323934393337333937'
[  +0.857217] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2542: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.016990] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Europe/Warsaw not found. Is the tzdata package installed?
[  +0.132397] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.007469] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000275] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000296] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000463] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000946] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000299] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000265] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000389] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.016621] hv_storvsc fd1d2cbd-ce7c-535c-966b-eb5f811c95f0: tag#255 cmd 0x2a status: scsi 0x0 srb 0x4 hv 0xc00000a1
[  +0.147399] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.002458] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000422] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000439] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000460] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001494] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000477] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000413] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000456] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.479719] systemd-journald[48]: File /var/log/journal/b4eae8821c494e89a7a8776db1ef01c2/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +0.334067] systemd-journald[48]: Failed to read journal file /var/log/journal/b4eae8821c494e89a7a8776db1ef01c2/user-1000.journal for rotation, trying to move it out of the way: Device or resource busy
[  +2.880137] WSL (207) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +7.609701] new mount options do not match the existing superblock, will be ignored
[  +0.001542] netlink: 'init': attribute type 4 has an invalid length.
[Apr14 12:07] blk_update_request: I/O error, dev sdd, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 0 prio class 0
[  +0.858076] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2542: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.004687] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Europe/Warsaw not found. Is the tzdata package installed?
[  +0.059308] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.003471] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000484] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000377] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000503] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001048] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000392] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000370] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000481] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.228371] Buffer I/O error on dev sdd, logical block 134184960, lost sync page write
[  +0.000485] JBD2: Error -5 detected when updating journal superblock for sdd-8.
[  +0.000276] Aborting journal on device sdd-8.
[  +0.000179] Buffer I/O error on dev sdd, logical block 134184960, lost sync page write
[  +0.000284] JBD2: Error -5 detected when updating journal superblock for sdd-8.
[  +0.000297] EXT4-fs error (device sdd): ext4_put_super:1204: comm wsl-bootstrap: Couldn't clean up the journal
[  +0.000676] EXT4-fs (sdd): Remounting filesystem read-only
[  +0.361520] new mount options do not match the existing superblock, will be ignored
[  +0.000546] netlink: 'init': attribute type 4 has an invalid length.
[Apr14 12:14] tmpfs: Unknown parameter 'noswap'
[  +4.718269] tmpfs: Unknown parameter 'noswap'


==> etcd [7f3b8de2903d] <==
{"level":"info","ts":"2025-04-14T12:18:06.927874Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-04-14T12:18:06.928000Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-14T12:18:06.928040Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-14T12:18:06.928047Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-14T12:18:06.928144Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-14T12:18:06.928831Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-04-14T12:18:06.928963Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-04-14T12:18:06.929540Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-04-14T12:18:06.929605Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-14T12:18:06.929640Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-14T12:18:06.929828Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-04-14T12:18:06.929874Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-04-14T12:18:07.848003Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-04-14T12:18:07.848073Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-04-14T12:18:07.848121Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-04-14T12:18:07.848141Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-04-14T12:18:07.848150Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-04-14T12:18:07.848157Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-04-14T12:18:07.848164Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-04-14T12:18:07.851720Z","caller":"etcdserver/server.go:2651","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-14T12:18:07.853034Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-04-14T12:18:07.853048Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-04-14T12:18:07.853080Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-04-14T12:18:07.853395Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-04-14T12:18:07.853423Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-04-14T12:18:07.853743Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-14T12:18:07.853775Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-14T12:18:07.853845Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-14T12:18:07.853901Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-14T12:18:07.854307Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-14T12:18:07.854452Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-04-14T12:18:07.854864Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-04-14T12:18:08.729741Z","caller":"traceutil/trace.go:171","msg":"trace[205802060] transaction","detail":"{read_only:false; response_revision:13; number_of_response:1; }","duration":"100.087828ms","start":"2025-04-14T12:18:08.629640Z","end":"2025-04-14T12:18:08.729728Z","steps":["trace[205802060] 'process raft request'  (duration: 96.104415ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T12:27:39.808750Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":638}
{"level":"info","ts":"2025-04-14T12:27:39.813368Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":638,"took":"4.339779ms","hash":3245951952,"current-db-size-bytes":1667072,"current-db-size":"1.7 MB","current-db-size-in-use-bytes":1667072,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-04-14T12:27:39.813422Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3245951952,"revision":638,"compact-revision":-1}
{"level":"info","ts":"2025-04-14T12:32:24.875560Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":960}
{"level":"info","ts":"2025-04-14T12:32:24.877988Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":960,"took":"2.184216ms","hash":3990918210,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1794048,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-04-14T12:32:24.878026Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3990918210,"revision":960,"compact-revision":638}
{"level":"info","ts":"2025-04-14T12:37:09.705491Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1236}
{"level":"info","ts":"2025-04-14T12:37:09.707836Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1236,"took":"2.086902ms","hash":628657545,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1650688,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-04-14T12:37:09.707876Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":628657545,"revision":1236,"compact-revision":960}
{"level":"info","ts":"2025-04-14T12:41:54.271772Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1479}
{"level":"info","ts":"2025-04-14T12:41:54.274160Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1479,"took":"2.098716ms","hash":1112034318,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1310720,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2025-04-14T12:41:54.274200Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1112034318,"revision":1479,"compact-revision":1236}
{"level":"info","ts":"2025-04-14T12:46:38.515162Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1715}
{"level":"info","ts":"2025-04-14T12:46:38.517067Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1715,"took":"1.630799ms","hash":3477796022,"current-db-size-bytes":1916928,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1433600,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-04-14T12:46:38.517107Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3477796022,"revision":1715,"compact-revision":1479}
{"level":"info","ts":"2025-04-14T12:51:22.571705Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2065}
{"level":"info","ts":"2025-04-14T12:51:22.575347Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2065,"took":"3.353463ms","hash":656297929,"current-db-size-bytes":2007040,"current-db-size":"2.0 MB","current-db-size-in-use-bytes":1728512,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-04-14T12:51:22.575381Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":656297929,"revision":2065,"compact-revision":1715}
{"level":"info","ts":"2025-04-14T12:56:06.484760Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2327}
{"level":"info","ts":"2025-04-14T12:56:06.487635Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2327,"took":"2.527155ms","hash":1615057358,"current-db-size-bytes":2244608,"current-db-size":"2.2 MB","current-db-size-in-use-bytes":1851392,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-04-14T12:56:06.487675Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1615057358,"revision":2327,"compact-revision":2065}
{"level":"info","ts":"2025-04-14T13:00:50.226174Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2662}
{"level":"info","ts":"2025-04-14T13:00:50.232633Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2662,"took":"5.999294ms","hash":803371952,"current-db-size-bytes":2473984,"current-db-size":"2.5 MB","current-db-size-in-use-bytes":2048000,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-04-14T13:00:50.232670Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":803371952,"revision":2662,"compact-revision":2327}
{"level":"info","ts":"2025-04-14T13:05:33.773472Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2953}
{"level":"info","ts":"2025-04-14T13:05:33.778445Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2953,"took":"4.632818ms","hash":2427508271,"current-db-size-bytes":2834432,"current-db-size":"2.8 MB","current-db-size-in-use-bytes":2338816,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2025-04-14T13:05:33.778494Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2427508271,"revision":2953,"compact-revision":2662}


==> kernel <==
 13:07:52 up  1:06,  0 users,  load average: 0.27, 0.35, 0.30
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [a01d4435ce67] <==
I0414 12:18:08.463753       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0414 12:18:08.463759       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0414 12:18:08.476454       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0414 12:18:08.476663       1 controller.go:90] Starting OpenAPI V3 controller
I0414 12:18:08.476681       1 naming_controller.go:294] Starting NamingConditionController
I0414 12:18:08.476715       1 establishing_controller.go:81] Starting EstablishingController
I0414 12:18:08.476730       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0414 12:18:08.476878       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0414 12:18:08.476893       1 crd_finalizer.go:269] Starting CRDFinalizer
I0414 12:18:08.500546       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0414 12:18:08.500605       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0414 12:18:08.500617       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0414 12:18:08.500626       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0414 12:18:08.500637       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0414 12:18:08.500690       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0414 12:18:08.537796       1 shared_informer.go:320] Caches are synced for node_authorizer
I0414 12:18:08.542882       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0414 12:18:08.542912       1 policy_source.go:240] refreshing policies
I0414 12:18:08.620749       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0414 12:18:08.620818       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0414 12:18:08.620996       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0414 12:18:08.620968       1 cache.go:39] Caches are synced for LocalAvailability controller
I0414 12:18:08.621108       1 shared_informer.go:320] Caches are synced for configmaps
I0414 12:18:08.621119       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0414 12:18:08.621128       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0414 12:18:08.621141       1 aggregator.go:171] initial CRD sync complete...
I0414 12:18:08.621148       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0414 12:18:08.621154       1 autoregister_controller.go:144] Starting autoregister controller
I0414 12:18:08.621164       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0414 12:18:08.621168       1 cache.go:39] Caches are synced for autoregister controller
I0414 12:18:08.621178       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0414 12:18:08.622167       1 controller.go:615] quota admission added evaluator for: namespaces
E0414 12:18:08.627348       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0414 12:18:08.831837       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0414 12:18:09.489076       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0414 12:18:09.493254       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0414 12:18:09.493288       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0414 12:18:09.815585       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0414 12:18:09.843954       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0414 12:18:09.932023       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0414 12:18:09.937613       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0414 12:18:09.938344       1 controller.go:615] quota admission added evaluator for: endpoints
I0414 12:18:09.941556       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0414 12:18:10.534186       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0414 12:18:11.036970       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0414 12:18:11.047357       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0414 12:18:11.053073       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0414 12:18:15.737733       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0414 12:18:15.784260       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0414 12:25:28.543181       1 alloc.go:330] "allocated clusterIPs" service="default/hello-node" clusterIPs={"IPv4":"10.111.39.206"}
I0414 12:28:54.077444       1 alloc.go:330] "allocated clusterIPs" service="default/hello-node" clusterIPs={"IPv4":"10.108.170.34"}
I0414 12:33:03.407485       1 alloc.go:330] "allocated clusterIPs" service="default/hello-node" clusterIPs={"IPv4":"10.111.178.67"}
I0414 12:45:17.466467       1 alloc.go:330] "allocated clusterIPs" service="default/hello-node" clusterIPs={"IPv4":"10.106.115.166"}
I0414 12:47:10.633309       1 alloc.go:330] "allocated clusterIPs" service="default/nginx-hello" clusterIPs={"IPv4":"10.108.233.147"}
I0414 12:55:56.823990       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/kubernetes-dashboard" clusterIPs={"IPv4":"10.109.167.91"}
I0414 12:55:56.879117       1 alloc.go:330] "allocated clusterIPs" service="kubernetes-dashboard/dashboard-metrics-scraper" clusterIPs={"IPv4":"10.100.248.183"}
I0414 12:59:40.425108       1 alloc.go:330] "allocated clusterIPs" service="default/joanna-web" clusterIPs={"IPv4":"10.101.167.39"}
I0414 13:01:55.404263       1 alloc.go:330] "allocated clusterIPs" service="default/joanna-web" clusterIPs={"IPv4":"10.97.137.240"}
I0414 13:04:02.849289       1 alloc.go:330] "allocated clusterIPs" service="default/joanna-web" clusterIPs={"IPv4":"10.102.25.148"}
I0414 13:07:04.038074       1 alloc.go:330] "allocated clusterIPs" service="default/joanna-web" clusterIPs={"IPv4":"10.97.94.98"}


==> kube-controller-manager [847ace557514] <==
I0414 12:55:56.890604       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="31.036µs"
I0414 12:55:56.891942       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="51.1551ms"
I0414 12:55:56.892016       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="34.296µs"
I0414 12:55:56.893696       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="20µs"
I0414 12:56:02.600639       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="4.888624ms"
I0414 12:56:02.600734       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="31.991µs"
I0414 12:56:17.530078       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 12:56:21.712692       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="5.124995ms"
I0414 12:56:21.712770       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="31.209µs"
I0414 12:56:46.537677       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 12:59:01.099397       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 12:59:40.428971       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="11.337219ms"
I0414 12:59:40.433798       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="4.722134ms"
I0414 12:59:40.433946       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="78.557µs"
I0414 12:59:40.436446       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="23.366µs"
I0414 12:59:43.026572       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="53.524µs"
I0414 12:59:54.604298       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="101.387µs"
I0414 13:00:10.598877       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="43.637µs"
I0414 13:00:21.939032       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="98.593µs"
I0414 13:00:35.941940       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="66.088µs"
I0414 13:00:45.316969       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="80.678µs"
I0414 13:00:50.603075       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="3.881µs"
I0414 13:01:55.407380       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="10.766919ms"
I0414 13:01:55.411558       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="4.12634ms"
I0414 13:01:55.411632       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="34.006µs"
I0414 13:01:55.411657       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="14.414µs"
I0414 13:01:55.416792       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="29.19µs"
I0414 13:01:57.949159       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="59.025µs"
I0414 13:02:10.336216       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="50.47µs"
I0414 13:02:25.339485       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="58.052µs"
I0414 13:02:38.704696       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="53.867µs"
I0414 13:02:42.249777       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 13:02:55.703923       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="41.341µs"
I0414 13:03:06.698175       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="48.173µs"
I0414 13:03:44.425788       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="77.803µs"
I0414 13:03:58.421090       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="119.684µs"
I0414 13:03:58.463176       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="3.503µs"
I0414 13:04:02.861783       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="15.907714ms"
I0414 13:04:02.865467       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="3.609881ms"
I0414 13:04:02.865545       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="28.133µs"
I0414 13:04:02.870891       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="29.116µs"
I0414 13:04:03.720911       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="41.486µs"
I0414 13:04:04.734245       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="43.62µs"
I0414 13:04:29.776256       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="56.966µs"
I0414 13:04:40.129628       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="62.835µs"
I0414 13:04:53.124538       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="55.721µs"
I0414 13:05:04.488901       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="60.47µs"
I0414 13:05:40.856753       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="68.916µs"
I0414 13:05:55.856823       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="38.229µs"
I0414 13:06:58.203862       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="5.093µs"
I0414 13:07:04.041933       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="10.209355ms"
I0414 13:07:04.047266       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="5.270472ms"
I0414 13:07:04.047356       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="38.293µs"
I0414 13:07:04.047373       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="8.917µs"
I0414 13:07:04.053239       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="20.881µs"
I0414 13:07:06.343459       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="40.433µs"
I0414 13:07:20.912742       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="86.989µs"
I0414 13:07:32.066003       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 13:07:32.277288       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="74.978µs"
I0414 13:07:45.285067       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/joanna-web-6b97746c44" duration="68.139µs"


==> kube-proxy [5b2f71a9b350] <==
I0414 12:18:16.934053       1 server_linux.go:66] "Using iptables proxy"
I0414 12:18:17.166644       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0414 12:18:17.166718       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0414 12:18:17.179914       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0414 12:18:17.179986       1 server_linux.go:170] "Using iptables Proxier"
I0414 12:18:17.181463       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0414 12:18:17.187777       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0414 12:18:17.191487       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0414 12:18:17.191561       1 server.go:497] "Version info" version="v1.32.0"
I0414 12:18:17.191592       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0414 12:18:17.195147       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0414 12:18:17.198504       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0414 12:18:17.199363       1 config.go:105] "Starting endpoint slice config controller"
I0414 12:18:17.199393       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0414 12:18:17.199426       1 config.go:199] "Starting service config controller"
I0414 12:18:17.199429       1 shared_informer.go:313] Waiting for caches to sync for service config
I0414 12:18:17.199442       1 config.go:329] "Starting node config controller"
I0414 12:18:17.199476       1 shared_informer.go:313] Waiting for caches to sync for node config
I0414 12:18:17.300716       1 shared_informer.go:320] Caches are synced for service config
I0414 12:18:17.300837       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0414 12:18:17.300874       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [180dcbd0e0b5] <==
I0414 12:18:07.265416       1 serving.go:386] Generated self-signed cert in-memory
W0414 12:18:08.525136       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0414 12:18:08.525193       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0414 12:18:08.525206       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0414 12:18:08.525214       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0414 12:18:08.624671       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0414 12:18:08.624704       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0414 12:18:08.626894       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0414 12:18:08.627225       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0414 12:18:08.627251       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0414 12:18:08.627293       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0414 12:18:08.628623       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0414 12:18:08.628690       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0414 12:18:08.628704       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0414 12:18:08.628631       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0414 12:18:08.628723       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0414 12:18:08.628729       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0414 12:18:08.628796       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0414 12:18:08.628813       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0414 12:18:08.628871       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0414 12:18:08.628906       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0414 12:18:08.628949       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0414 12:18:08.628952       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0414 12:18:08.628969       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0414 12:18:08.628971       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0414 12:18:08.628994       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0414 12:18:08.629079       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0414 12:18:08.629085       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0414 12:18:08.629032       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0414 12:18:08.629094       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0414 12:18:08.629103       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0414 12:18:08.629033       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0414 12:18:08.629151       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0414 12:18:08.629065       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0414 12:18:08.629160       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0414 12:18:08.629592       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0414 12:18:08.629630       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0414 12:18:08.629858       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0414 12:18:08.629894       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0414 12:18:08.630691       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0414 12:18:08.630712       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0414 12:18:08.631427       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0414 12:18:08.631503       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0414 12:18:09.434232       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0414 12:18:09.434279       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0414 12:18:09.505077       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0414 12:18:09.505121       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0414 12:18:09.545694       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0414 12:18:09.545738       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0414 12:18:09.652601       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0414 12:18:09.652649       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
I0414 12:18:09.927902       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Apr 14 13:03:33 minikube kubelet[2582]: E0414 13:03:33.828902    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ErrImagePull: \"Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-f92vd" podUID="91ee6693-3020-4871-892f-4e4fa9da5c51"
Apr 14 13:03:44 minikube kubelet[2582]: E0414 13:03:44.411980    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ImagePullBackOff: \"Back-off pulling image \\\"joanna-web\\\": ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-f92vd" podUID="91ee6693-3020-4871-892f-4e4fa9da5c51"
Apr 14 13:03:58 minikube kubelet[2582]: E0414 13:03:58.413537    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ImagePullBackOff: \"Back-off pulling image \\\"joanna-web\\\": ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-f92vd" podUID="91ee6693-3020-4871-892f-4e4fa9da5c51"
Apr 14 13:03:59 minikube kubelet[2582]: I0414 13:03:59.943424    2582 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-27qmx\" (UniqueName: \"kubernetes.io/projected/91ee6693-3020-4871-892f-4e4fa9da5c51-kube-api-access-27qmx\") pod \"91ee6693-3020-4871-892f-4e4fa9da5c51\" (UID: \"91ee6693-3020-4871-892f-4e4fa9da5c51\") "
Apr 14 13:03:59 minikube kubelet[2582]: I0414 13:03:59.950851    2582 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/91ee6693-3020-4871-892f-4e4fa9da5c51-kube-api-access-27qmx" (OuterVolumeSpecName: "kube-api-access-27qmx") pod "91ee6693-3020-4871-892f-4e4fa9da5c51" (UID: "91ee6693-3020-4871-892f-4e4fa9da5c51"). InnerVolumeSpecName "kube-api-access-27qmx". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Apr 14 13:04:00 minikube kubelet[2582]: I0414 13:04:00.044281    2582 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-27qmx\" (UniqueName: \"kubernetes.io/projected/91ee6693-3020-4871-892f-4e4fa9da5c51-kube-api-access-27qmx\") on node \"minikube\" DevicePath \"\""
Apr 14 13:04:01 minikube kubelet[2582]: I0414 13:04:01.425999    2582 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="91ee6693-3020-4871-892f-4e4fa9da5c51" path="/var/lib/kubelet/pods/91ee6693-3020-4871-892f-4e4fa9da5c51/volumes"
Apr 14 13:04:02 minikube kubelet[2582]: I0414 13:04:02.874428    2582 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-28lbt\" (UniqueName: \"kubernetes.io/projected/085a3d5d-7748-485d-a110-55887e303809-kube-api-access-28lbt\") pod \"joanna-web-6b97746c44-rp45l\" (UID: \"085a3d5d-7748-485d-a110-55887e303809\") " pod="default/joanna-web-6b97746c44-rp45l"
Apr 14 13:04:03 minikube kubelet[2582]: E0414 13:04:03.338857    2582 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="joanna-web:latest"
Apr 14 13:04:03 minikube kubelet[2582]: E0414 13:04:03.338973    2582 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="joanna-web:latest"
Apr 14 13:04:03 minikube kubelet[2582]: E0414 13:04:03.339158    2582 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:joanna-web,Image:joanna-web,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-28lbt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod joanna-web-6b97746c44-rp45l_default(085a3d5d-7748-485d-a110-55887e303809): ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Apr 14 13:04:03 minikube kubelet[2582]: E0414 13:04:03.340414    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ErrImagePull: \"Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-rp45l" podUID="085a3d5d-7748-485d-a110-55887e303809"
Apr 14 13:04:04 minikube kubelet[2582]: E0414 13:04:04.335383    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ImagePullBackOff: \"Back-off pulling image \\\"joanna-web\\\": ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-rp45l" podUID="085a3d5d-7748-485d-a110-55887e303809"
Apr 14 13:04:04 minikube kubelet[2582]: I0414 13:04:04.725668    2582 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="234a660c43bc296c9b86cd6075652adbfff8924e5eded939033d6fc9e6fa07c0"
Apr 14 13:04:04 minikube kubelet[2582]: E0414 13:04:04.726012    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ImagePullBackOff: \"Back-off pulling image \\\"joanna-web\\\": ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-rp45l" podUID="085a3d5d-7748-485d-a110-55887e303809"
Apr 14 13:04:17 minikube kubelet[2582]: E0414 13:04:17.467903    2582 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="joanna-web:latest"
Apr 14 13:04:17 minikube kubelet[2582]: E0414 13:04:17.467967    2582 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="joanna-web:latest"
Apr 14 13:04:17 minikube kubelet[2582]: E0414 13:04:17.468100    2582 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:joanna-web,Image:joanna-web,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-28lbt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod joanna-web-6b97746c44-rp45l_default(085a3d5d-7748-485d-a110-55887e303809): ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Apr 14 13:04:17 minikube kubelet[2582]: E0414 13:04:17.469341    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ErrImagePull: \"Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-rp45l" podUID="085a3d5d-7748-485d-a110-55887e303809"
Apr 14 13:04:29 minikube kubelet[2582]: E0414 13:04:29.765290    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ImagePullBackOff: \"Back-off pulling image \\\"joanna-web\\\": ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-rp45l" podUID="085a3d5d-7748-485d-a110-55887e303809"
Apr 14 13:04:41 minikube kubelet[2582]: E0414 13:04:41.880275    2582 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="joanna-web:latest"
Apr 14 13:04:41 minikube kubelet[2582]: E0414 13:04:41.880364    2582 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="joanna-web:latest"
Apr 14 13:04:41 minikube kubelet[2582]: E0414 13:04:41.880460    2582 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:joanna-web,Image:joanna-web,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-28lbt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod joanna-web-6b97746c44-rp45l_default(085a3d5d-7748-485d-a110-55887e303809): ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Apr 14 13:04:41 minikube kubelet[2582]: E0414 13:04:41.881900    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ErrImagePull: \"Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-rp45l" podUID="085a3d5d-7748-485d-a110-55887e303809"
Apr 14 13:04:53 minikube kubelet[2582]: E0414 13:04:53.116376    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ImagePullBackOff: \"Back-off pulling image \\\"joanna-web\\\": ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-rp45l" podUID="085a3d5d-7748-485d-a110-55887e303809"
Apr 14 13:05:04 minikube kubelet[2582]: E0414 13:05:04.478382    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ImagePullBackOff: \"Back-off pulling image \\\"joanna-web\\\": ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-rp45l" podUID="085a3d5d-7748-485d-a110-55887e303809"
Apr 14 13:05:19 minikube kubelet[2582]: E0414 13:05:19.480206    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ImagePullBackOff: \"Back-off pulling image \\\"joanna-web\\\": ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-rp45l" podUID="085a3d5d-7748-485d-a110-55887e303809"
Apr 14 13:05:30 minikube kubelet[2582]: E0414 13:05:30.458722    2582 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="joanna-web:latest"
Apr 14 13:05:30 minikube kubelet[2582]: E0414 13:05:30.458813    2582 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="joanna-web:latest"
Apr 14 13:05:30 minikube kubelet[2582]: E0414 13:05:30.458929    2582 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:joanna-web,Image:joanna-web,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-28lbt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod joanna-web-6b97746c44-rp45l_default(085a3d5d-7748-485d-a110-55887e303809): ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Apr 14 13:05:30 minikube kubelet[2582]: E0414 13:05:30.460256    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ErrImagePull: \"Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-rp45l" podUID="085a3d5d-7748-485d-a110-55887e303809"
Apr 14 13:05:40 minikube kubelet[2582]: E0414 13:05:40.840814    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ImagePullBackOff: \"Back-off pulling image \\\"joanna-web\\\": ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-rp45l" podUID="085a3d5d-7748-485d-a110-55887e303809"
Apr 14 13:05:55 minikube kubelet[2582]: E0414 13:05:55.840735    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ImagePullBackOff: \"Back-off pulling image \\\"joanna-web\\\": ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-rp45l" podUID="085a3d5d-7748-485d-a110-55887e303809"
Apr 14 13:06:09 minikube kubelet[2582]: E0414 13:06:09.200023    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ImagePullBackOff: \"Back-off pulling image \\\"joanna-web\\\": ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-rp45l" podUID="085a3d5d-7748-485d-a110-55887e303809"
Apr 14 13:06:22 minikube kubelet[2582]: E0414 13:06:22.192643    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ImagePullBackOff: \"Back-off pulling image \\\"joanna-web\\\": ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-rp45l" podUID="085a3d5d-7748-485d-a110-55887e303809"
Apr 14 13:06:31 minikube kubelet[2582]: E0414 13:06:31.550761    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ImagePullBackOff: \"Back-off pulling image \\\"joanna-web\\\": ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-rp45l" podUID="085a3d5d-7748-485d-a110-55887e303809"
Apr 14 13:06:44 minikube kubelet[2582]: E0414 13:06:44.550816    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ImagePullBackOff: \"Back-off pulling image \\\"joanna-web\\\": ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-rp45l" podUID="085a3d5d-7748-485d-a110-55887e303809"
Apr 14 13:06:59 minikube kubelet[2582]: E0414 13:06:59.571943    2582 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="joanna-web:latest"
Apr 14 13:06:59 minikube kubelet[2582]: E0414 13:06:59.572011    2582 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="joanna-web:latest"
Apr 14 13:06:59 minikube kubelet[2582]: E0414 13:06:59.572115    2582 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:joanna-web,Image:joanna-web,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-28lbt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod joanna-web-6b97746c44-rp45l_default(085a3d5d-7748-485d-a110-55887e303809): ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Apr 14 13:06:59 minikube kubelet[2582]: E0414 13:06:59.573415    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ErrImagePull: \"Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-rp45l" podUID="085a3d5d-7748-485d-a110-55887e303809"
Apr 14 13:07:00 minikube kubelet[2582]: I0414 13:07:00.907645    2582 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-28lbt\" (UniqueName: \"kubernetes.io/projected/085a3d5d-7748-485d-a110-55887e303809-kube-api-access-28lbt\") pod \"085a3d5d-7748-485d-a110-55887e303809\" (UID: \"085a3d5d-7748-485d-a110-55887e303809\") "
Apr 14 13:07:00 minikube kubelet[2582]: I0414 13:07:00.910123    2582 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/085a3d5d-7748-485d-a110-55887e303809-kube-api-access-28lbt" (OuterVolumeSpecName: "kube-api-access-28lbt") pod "085a3d5d-7748-485d-a110-55887e303809" (UID: "085a3d5d-7748-485d-a110-55887e303809"). InnerVolumeSpecName "kube-api-access-28lbt". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Apr 14 13:07:01 minikube kubelet[2582]: I0414 13:07:01.008142    2582 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-28lbt\" (UniqueName: \"kubernetes.io/projected/085a3d5d-7748-485d-a110-55887e303809-kube-api-access-28lbt\") on node \"minikube\" DevicePath \"\""
Apr 14 13:07:01 minikube kubelet[2582]: I0414 13:07:01.906233    2582 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="085a3d5d-7748-485d-a110-55887e303809" path="/var/lib/kubelet/pods/085a3d5d-7748-485d-a110-55887e303809/volumes"
Apr 14 13:07:04 minikube kubelet[2582]: I0414 13:07:04.152877    2582 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mmxlp\" (UniqueName: \"kubernetes.io/projected/6cd17bfb-57f9-4bc1-9ff8-ba5d134cf349-kube-api-access-mmxlp\") pod \"joanna-web-6b97746c44-hs4pc\" (UID: \"6cd17bfb-57f9-4bc1-9ff8-ba5d134cf349\") " pod="default/joanna-web-6b97746c44-hs4pc"
Apr 14 13:07:06 minikube kubelet[2582]: E0414 13:07:06.169428    2582 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="joanna-web:latest"
Apr 14 13:07:06 minikube kubelet[2582]: E0414 13:07:06.169499    2582 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="joanna-web:latest"
Apr 14 13:07:06 minikube kubelet[2582]: E0414 13:07:06.169613    2582 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:joanna-web,Image:joanna-web,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mmxlp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod joanna-web-6b97746c44-hs4pc_default(6cd17bfb-57f9-4bc1-9ff8-ba5d134cf349): ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Apr 14 13:07:06 minikube kubelet[2582]: E0414 13:07:06.171086    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ErrImagePull: \"Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-hs4pc" podUID="6cd17bfb-57f9-4bc1-9ff8-ba5d134cf349"
Apr 14 13:07:06 minikube kubelet[2582]: E0414 13:07:06.336108    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ImagePullBackOff: \"Back-off pulling image \\\"joanna-web\\\": ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-hs4pc" podUID="6cd17bfb-57f9-4bc1-9ff8-ba5d134cf349"
Apr 14 13:07:21 minikube kubelet[2582]: E0414 13:07:21.035018    2582 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="joanna-web:latest"
Apr 14 13:07:21 minikube kubelet[2582]: E0414 13:07:21.035086    2582 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="joanna-web:latest"
Apr 14 13:07:21 minikube kubelet[2582]: E0414 13:07:21.035203    2582 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:joanna-web,Image:joanna-web,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mmxlp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod joanna-web-6b97746c44-hs4pc_default(6cd17bfb-57f9-4bc1-9ff8-ba5d134cf349): ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Apr 14 13:07:21 minikube kubelet[2582]: E0414 13:07:21.036496    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ErrImagePull: \"Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-hs4pc" podUID="6cd17bfb-57f9-4bc1-9ff8-ba5d134cf349"
Apr 14 13:07:32 minikube kubelet[2582]: E0414 13:07:32.268334    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ImagePullBackOff: \"Back-off pulling image \\\"joanna-web\\\": ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-hs4pc" podUID="6cd17bfb-57f9-4bc1-9ff8-ba5d134cf349"
Apr 14 13:07:46 minikube kubelet[2582]: E0414 13:07:46.892364    2582 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="joanna-web:latest"
Apr 14 13:07:46 minikube kubelet[2582]: E0414 13:07:46.892428    2582 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="joanna-web:latest"
Apr 14 13:07:46 minikube kubelet[2582]: E0414 13:07:46.892537    2582 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:joanna-web,Image:joanna-web,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mmxlp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod joanna-web-6b97746c44-hs4pc_default(6cd17bfb-57f9-4bc1-9ff8-ba5d134cf349): ErrImagePull: Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Apr 14 13:07:46 minikube kubelet[2582]: E0414 13:07:46.893810    2582 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"joanna-web\" with ErrImagePull: \"Error response from daemon: pull access denied for joanna-web, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/joanna-web-6b97746c44-hs4pc" podUID="6cd17bfb-57f9-4bc1-9ff8-ba5d134cf349"


==> kubernetes-dashboard [cbfda574a2b1] <==
2025/04/14 12:57:08 received 0 resources from sidecar instead of 4
2025/04/14 12:57:08 [2025-04-14T12:57:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/14 12:57:08 received 0 resources from sidecar instead of 4
2025/04/14 12:57:08 Getting pod metrics
2025/04/14 12:57:08 received 0 resources from sidecar instead of 4
2025/04/14 12:57:08 [2025-04-14T12:57:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/14 12:57:08 received 0 resources from sidecar instead of 4
2025/04/14 12:57:08 received 0 resources from sidecar instead of 4
2025/04/14 12:57:08 Skipping metric because of error: Metric label not set.
2025/04/14 12:57:08 Skipping metric because of error: Metric label not set.
2025/04/14 12:57:08 Skipping metric because of error: Metric label not set.
2025/04/14 12:57:08 Skipping metric because of error: Metric label not set.
2025/04/14 12:57:08 Skipping metric because of error: Metric label not set.
2025/04/14 12:57:08 Skipping metric because of error: Metric label not set.
2025/04/14 12:57:08 Skipping metric because of error: Metric label not set.
2025/04/14 12:57:08 Skipping metric because of error: Metric label not set.
2025/04/14 12:57:08 [2025-04-14T12:57:08Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/14 12:57:13 Getting list of all pods in the cluster
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/14 12:57:13 Getting list of all deployments in the cluster
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/14 12:57:13 Getting list of all jobs in the cluster
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/04/14 12:57:13 Getting list of namespaces
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/14 12:57:13 Getting list of all cron jobs in the cluster
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/14 12:57:13 received 0 resources from sidecar instead of 4
2025/04/14 12:57:13 received 0 resources from sidecar instead of 4
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/14 12:57:13 Getting list of all replica sets in the cluster
2025/04/14 12:57:13 received 0 resources from sidecar instead of 4
2025/04/14 12:57:13 Getting pod metrics
2025/04/14 12:57:13 received 0 resources from sidecar instead of 4
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/14 12:57:13 Getting list of all pet sets in the cluster
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/04/14 12:57:13 Getting list of all replication controllers in the cluster
2025/04/14 12:57:13 received 0 resources from sidecar instead of 4
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/14 12:57:13 received 0 resources from sidecar instead of 4
2025/04/14 12:57:13 received 0 resources from sidecar instead of 4
2025/04/14 12:57:13 Skipping metric because of error: Metric label not set.
2025/04/14 12:57:13 Skipping metric because of error: Metric label not set.
2025/04/14 12:57:13 Skipping metric because of error: Metric label not set.
2025/04/14 12:57:13 Skipping metric because of error: Metric label not set.
2025/04/14 12:57:13 Skipping metric because of error: Metric label not set.
2025/04/14 12:57:13 Skipping metric because of error: Metric label not set.
2025/04/14 12:57:13 Skipping metric because of error: Metric label not set.
2025/04/14 12:57:13 Skipping metric because of error: Metric label not set.
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Outcoming response to 127.0.0.1 with 200 status code
2025/04/14 12:57:13 received 0 resources from sidecar instead of 4
2025/04/14 12:57:13 [2025-04-14T12:57:13Z] Outcoming response to 127.0.0.1 with 200 status code


==> storage-provisioner [0b976504b3f6] <==
I0414 12:18:37.930485       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0414 12:18:37.937490       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0414 12:18:37.937529       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0414 12:18:37.943771       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0414 12:18:37.943901       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_5db8de1c-998b-4a86-8752-b175f02cea8a!
I0414 12:18:37.943893       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"025ed792-d9c4-47d9-8fc9-150a105fba5f", APIVersion:"v1", ResourceVersion:"420", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_5db8de1c-998b-4a86-8752-b175f02cea8a became leader
I0414 12:18:38.044360       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_5db8de1c-998b-4a86-8752-b175f02cea8a!


==> storage-provisioner [884ce9f3cfc3] <==
I0414 12:18:17.107404       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0414 12:18:37.731926       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

